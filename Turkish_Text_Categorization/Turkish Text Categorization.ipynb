{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "human-fossil",
   "metadata": {},
   "source": [
    "In this notebook, we are going to categorize reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "monetary-palestine",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import PorterStemmer\n",
    "from snowballstemmer import TurkishStemmer\n",
    "\n",
    "import jpype\n",
    "from typing import List\n",
    "from jpype import JClass, JString, getDefaultJVMPath, shutdownJVM, startJVM, java\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "diverse-green",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>siyaset</td>\n",
       "      <td>3 milyon ile ön seçim vaadi mhp nin 10 olağan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>siyaset</td>\n",
       "      <td>mesut_yılmaz yüce_divan da ceza alabilirdi pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>siyaset</td>\n",
       "      <td>disko lar kaldırılıyor başbakan_yardımcısı ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>siyaset</td>\n",
       "      <td>sarıgül anayasa_mahkemesi ne gidiyor mustafa_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>siyaset</td>\n",
       "      <td>erdoğan idamın bir haklılık sebebi var demek ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                               text\n",
       "0  siyaset    3 milyon ile ön seçim vaadi mhp nin 10 olağan...\n",
       "1  siyaset    mesut_yılmaz yüce_divan da ceza alabilirdi pr...\n",
       "2  siyaset    disko lar kaldırılıyor başbakan_yardımcısı ar...\n",
       "3  siyaset    sarıgül anayasa_mahkemesi ne gidiyor mustafa_...\n",
       "4  siyaset    erdoğan idamın bir haklılık sebebi var demek ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('7allV03.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-sword",
   "metadata": {},
   "source": [
    "#### Quick Overview of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "french-herald",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4900 entries, 0 to 4899\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   category  4900 non-null   object\n",
      " 1   text      4900 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 76.7+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "competitive-reform",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "siyaset       700\n",
       "dunya         700\n",
       "ekonomi       700\n",
       "kultur        700\n",
       "saglik        700\n",
       "spor          700\n",
       "teknoloji     700\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ahead-compensation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 categories\n"
     ]
    }
   ],
   "source": [
    "print('There are', len(data['category'].value_counts()), 'categories')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "retained-definition",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\burak\\AppData\\Roaming\\Python\\Python39\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='category', ylabel='count'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(data['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historic-forum",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-marriage",
   "metadata": {},
   "source": [
    "#### Data cleaning\n",
    "\n",
    "-> Removing of punctuations, numbers, stopwords, urls etc.\n",
    "\n",
    "-> Converting uppercase to lower\n",
    "\n",
    "-> Tokenization\n",
    "\n",
    "-> Lemmatization or Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-victor",
   "metadata": {},
   "source": [
    "#### SENTENCE TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "incorporate-routine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>text_sentence_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>siyaset</td>\n",
       "      <td>3 milyon ile ön seçim vaadi mhp nin 10 olağan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>siyaset</td>\n",
       "      <td>mesut_yılmaz yüce_divan da ceza alabilirdi pr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>siyaset</td>\n",
       "      <td>disko lar kaldırılıyor başbakan_yardımcısı ar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>siyaset</td>\n",
       "      <td>sarıgül anayasa_mahkemesi ne gidiyor mustafa_...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>siyaset</td>\n",
       "      <td>erdoğan idamın bir haklılık sebebi var demek ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                               text  \\\n",
       "0  siyaset    3 milyon ile ön seçim vaadi mhp nin 10 olağan...   \n",
       "1  siyaset    mesut_yılmaz yüce_divan da ceza alabilirdi pr...   \n",
       "2  siyaset    disko lar kaldırılıyor başbakan_yardımcısı ar...   \n",
       "3  siyaset    sarıgül anayasa_mahkemesi ne gidiyor mustafa_...   \n",
       "4  siyaset    erdoğan idamın bir haklılık sebebi var demek ...   \n",
       "\n",
       "   text_sentence_count  \n",
       "0                    1  \n",
       "1                    1  \n",
       "2                    1  \n",
       "3                    1  \n",
       "4                    1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text_sentence_count'] = data['text'].apply(sent_tokenize).tolist()\n",
    "data['text_sentence_count'] = data['text_sentence_count'].apply(len)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "stainless-newcastle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     4349\n",
       "2      434\n",
       "3       70\n",
       "4       18\n",
       "5       16\n",
       "6        5\n",
       "22       2\n",
       "7        2\n",
       "23       1\n",
       "8        1\n",
       "10       1\n",
       "12       1\n",
       "Name: text_sentence_count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text_sentence_count'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "breathing-fortune",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is mostly no punctuations to end of the sentences, so remove that column.\n",
    "data.drop('text_sentence_count', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-overhead",
   "metadata": {},
   "source": [
    "#### REMOVE UNNECESSARY CHARACTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "composed-oracle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acaba', 'ama', 'aslında', 'az', 'bazı', 'belki', 'biri', 'birkaç', 'birşey', 'biz', 'bu', 'çok', 'çünkü', 'da', 'daha', 'de', 'defa', 'diye', 'eğer', 'en', 'gibi', 'hem', 'hep', 'hepsi', 'her', 'hiç', 'için', 'ile', 'ise', 'kez', 'ki', 'kim', 'mı', 'mu', 'mü', 'nasıl', 'ne', 'neden', 'nerde', 'nerede', 'nereye', 'niçin', 'niye', 'o', 'sanki', 'şey', 'siz', 'şu', 'tüm', 've', 'veya', 'ya', 'yani', 'a', 'acaba', 'altÄ±', 'altmÄ±ÅŸ', 'ama', 'ancak', 'arada', 'artÄ±k', 'asla', 'aslÄ±nda', 'aslÄ±nda', 'ayrÄ±ca', 'az', 'bana', 'bazen', 'bazÄ±', 'bazÄ±larÄ±', 'belki', 'ben', 'benden', 'beni', 'benim', 'beri', 'beÅŸ', 'bile', 'bilhassa', 'bin', 'bir', 'biraz', 'birÃ§oÄŸu', 'birÃ§ok', 'biri', 'birisi', 'birkaÃ§', 'birÅŸey', 'biz', 'bizden', 'bize', 'bizi', 'bizim', 'bÃ¶yle', 'bÃ¶ylece', 'bu', 'buna', 'bunda', 'bundan', 'bunlar', 'bunlarÄ±', 'bunlarÄ±n', 'bunu', 'bunun', 'burada', 'bÃ¼tÃ¼n', 'Ã§oÄŸu', 'Ã§oÄŸunu', 'Ã§ok', 'Ã§Ã¼nkÃ¼', 'da', 'daha', 'dahi', 'dan', 'de', 'defa', 'deÄŸil', 'diÄŸer', 'diÄŸeri', 'diÄŸerleri', 'diye', 'doksan', 'dokuz', 'dolayÄ±', 'dolayÄ±sÄ±yla', 'dÃ¶rt', 'e', 'edecek', 'eden', 'ederek', 'edilecek', 'ediliyor', 'edilmesi', 'ediyor', 'eÄŸer', 'elbette', 'elli', 'en', 'etmesi', 'etti', 'ettiÄŸi', 'ettiÄŸini', 'fakat', 'falan', 'filan', 'gene', 'gereÄŸi', 'gerek', 'gibi', 'gÃ¶re', 'hala', 'halde', 'halen', 'hangi', 'hangisi', 'hani', 'hatta', 'hem', 'henÃ¼z', 'hep', 'hepsi', 'her', 'herhangi', 'herkes', 'herkese', 'herkesi', 'herkesin', 'hiÃ§', 'hiÃ§bir', 'hiÃ§biri', 'i', 'Ä±', 'iÃ§in', 'iÃ§inde', 'iki', 'ile', 'ilgili', 'ise', 'iÅŸte', 'itibaren', 'itibariyle', 'kaÃ§', 'kadar', 'karÅŸÄ±n', 'kendi', 'kendilerine', 'kendine', 'kendini', 'kendisi', 'kendisine', 'kendisini', 'kez', 'ki', 'kim', 'kime', 'kimi', 'kimin', 'kimisi', 'kimse', 'kÄ±rk', 'madem', 'mi', 'mÄ±', 'milyar', 'milyon', 'mu', 'mÃ¼', 'nasÄ±l', 'ne', 'neden', 'nedenle', 'nerde', 'nerede', 'nereye', 'neyse', 'niÃ§in', 'nin', 'nÄ±n', 'niye', 'nun', 'nÃ¼n', 'o', 'Ã¶bÃ¼r', 'olan', 'olarak', 'oldu', 'olduÄŸu', 'olduÄŸunu', 'olduklarÄ±nÄ±', 'olmadÄ±', 'olmadÄ±ÄŸÄ±', 'olmak', 'olmasÄ±', 'olmayan', 'olmaz', 'olsa', 'olsun', 'olup', 'olur', 'olur', 'olursa', 'oluyor', 'on', 'Ã¶n', 'ona', 'Ã¶nce', 'ondan', 'onlar', 'onlara', 'onlardan', 'onlarÄ±', 'onlarÄ±n', 'onu', 'onun', 'orada', 'Ã¶te', 'Ã¶tÃ¼rÃ¼', 'otuz', 'Ã¶yle', 'oysa', 'pek', 'raÄŸmen', 'sana', 'sanki', 'sanki', 'ÅŸayet', 'ÅŸekilde', 'sekiz', 'seksen', 'sen', 'senden', 'seni', 'senin', 'ÅŸey', 'ÅŸeyden', 'ÅŸeye', 'ÅŸeyi', 'ÅŸeyler', 'ÅŸimdi', 'siz', 'siz', 'sizden', 'sizden', 'size', 'sizi', 'sizi', 'sizin', 'sizin', 'sonra', 'ÅŸÃ¶yle', 'ÅŸu', 'ÅŸuna', 'ÅŸunlarÄ±', 'ÅŸunu', 'ta', 'tabii', 'tam', 'tamam', 'tamamen', 'tarafÄ±ndan', 'trilyon', 'tÃ¼m', 'tÃ¼mÃ¼', 'u', 'Ã¼', 'Ã¼Ã§', 'un', 'Ã¼n', 'Ã¼zere', 'var', 'vardÄ±', 've', 'veya', 'ya', 'yani', 'yapacak', 'yapÄ±lan', 'yapÄ±lmasÄ±', 'yapÄ±yor', 'yapmak', 'yaptÄ±', 'yaptÄ±ÄŸÄ±', 'yaptÄ±ÄŸÄ±nÄ±', 'yaptÄ±klarÄ±', 'ye', 'yedi', 'yerine', 'yetmiÅŸ', 'yi', 'yÄ±', 'yine', 'yirmi', 'yoksa', 'yu', 'yÃ¼z', 'zaten', 'zira', 'zxtest']\n"
     ]
    }
   ],
   "source": [
    "# create a set of stopwords\n",
    "stopwords_set = stopwords.words('turkish')\n",
    "for line in open('turkish_stopwords.txt','r').readlines():\n",
    "    stopwords_set.append(line.strip())\n",
    "    \n",
    "print(stopwords_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "persistent-detection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a set of punctuations\n",
    "punctuation_set = string.punctuation\n",
    "punctuation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "human-asbestos",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of urls in tweets  6\n",
      "Number of html tag in tweets  1\n",
      "Number of digits in tweets  49168\n",
      "Number of punc in tweets  26242\n"
     ]
    }
   ],
   "source": [
    "# Check has some unnecessary words. Find if exists and count\n",
    "re_urls = r'http\\S+'\n",
    "re_html = r'<.*?>'\n",
    "re_digits = r'\\d+'\n",
    "re_punc = r'[^\\w\\s\\d]'\n",
    "\n",
    "print(\"Number of urls in tweets \", data['text'].str.count(re_urls).sum())\n",
    "print(\"Number of html tag in tweets \", data['text'].str.count(re_html).sum())\n",
    "print(\"Number of digits in tweets \", data['text'].str.count(re_digits).sum())\n",
    "print(\"Number of punc in tweets \", data['text'].str.count(re_punc).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "strong-bench",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    #lowercase operation\n",
    "    text = text.lower()\n",
    "    #remove urls\n",
    "    text = re.sub(re_urls, '', text)\n",
    "    #remove html\n",
    "    text = re.sub(re_html, '', text)\n",
    "    #remove digits\n",
    "    text = re.sub(re_digits, '', text)\n",
    "    # remove punctuations\n",
    "    text = \"\".join(list(map(lambda x:x if x not in punctuation_set else \" \", text)))\n",
    "    #remove stop words \n",
    "    text = text.split()\n",
    "    text = ' '.join([word for word in text if not word in stopwords_set])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "measured-today",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>siyaset</td>\n",
       "      <td>3 milyon ile ön seçim vaadi mhp nin 10 olağan...</td>\n",
       "      <td>ön seçim vaadi mhp olağan büyük kurultayı nda ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>siyaset</td>\n",
       "      <td>mesut_yılmaz yüce_divan da ceza alabilirdi pr...</td>\n",
       "      <td>mesut yılmaz yüce divan ceza alabilirdi prof d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>siyaset</td>\n",
       "      <td>disko lar kaldırılıyor başbakan_yardımcısı ar...</td>\n",
       "      <td>disko lar kaldırılıyor başbakan yardımcısı arı...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>siyaset</td>\n",
       "      <td>sarıgül anayasa_mahkemesi ne gidiyor mustafa_...</td>\n",
       "      <td>sarıgül anayasa mahkemesi gidiyor mustafa sarı...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>siyaset</td>\n",
       "      <td>erdoğan idamın bir haklılık sebebi var demek ...</td>\n",
       "      <td>erdoğan idamın haklılık sebebi demek yeri geld...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                               text  \\\n",
       "0  siyaset    3 milyon ile ön seçim vaadi mhp nin 10 olağan...   \n",
       "1  siyaset    mesut_yılmaz yüce_divan da ceza alabilirdi pr...   \n",
       "2  siyaset    disko lar kaldırılıyor başbakan_yardımcısı ar...   \n",
       "3  siyaset    sarıgül anayasa_mahkemesi ne gidiyor mustafa_...   \n",
       "4  siyaset    erdoğan idamın bir haklılık sebebi var demek ...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  ön seçim vaadi mhp olağan büyük kurultayı nda ...  \n",
       "1  mesut yılmaz yüce divan ceza alabilirdi prof d...  \n",
       "2  disko lar kaldırılıyor başbakan yardımcısı arı...  \n",
       "3  sarıgül anayasa mahkemesi gidiyor mustafa sarı...  \n",
       "4  erdoğan idamın haklılık sebebi demek yeri geld...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['cleaned_text'] = data['text'].apply(lambda x: clean(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-harassment",
   "metadata": {},
   "source": [
    "#### WORD TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "plain-yugoslavia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>text_word</th>\n",
       "      <th>text_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>siyaset</td>\n",
       "      <td>3 milyon ile ön seçim vaadi mhp nin 10 olağan...</td>\n",
       "      <td>ön seçim vaadi mhp olağan büyük kurultayı nda ...</td>\n",
       "      <td>[ön, seçim, vaadi, mhp, olağan, büyük, kurulta...</td>\n",
       "      <td>719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>siyaset</td>\n",
       "      <td>mesut_yılmaz yüce_divan da ceza alabilirdi pr...</td>\n",
       "      <td>mesut yılmaz yüce divan ceza alabilirdi prof d...</td>\n",
       "      <td>[mesut, yılmaz, yüce, divan, ceza, alabilirdi,...</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>siyaset</td>\n",
       "      <td>disko lar kaldırılıyor başbakan_yardımcısı ar...</td>\n",
       "      <td>disko lar kaldırılıyor başbakan yardımcısı arı...</td>\n",
       "      <td>[disko, lar, kaldırılıyor, başbakan, yardımcıs...</td>\n",
       "      <td>575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>siyaset</td>\n",
       "      <td>sarıgül anayasa_mahkemesi ne gidiyor mustafa_...</td>\n",
       "      <td>sarıgül anayasa mahkemesi gidiyor mustafa sarı...</td>\n",
       "      <td>[sarıgül, anayasa, mahkemesi, gidiyor, mustafa...</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>siyaset</td>\n",
       "      <td>erdoğan idamın bir haklılık sebebi var demek ...</td>\n",
       "      <td>erdoğan idamın haklılık sebebi demek yeri geld...</td>\n",
       "      <td>[erdoğan, idamın, haklılık, sebebi, demek, yer...</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                               text  \\\n",
       "0  siyaset    3 milyon ile ön seçim vaadi mhp nin 10 olağan...   \n",
       "1  siyaset    mesut_yılmaz yüce_divan da ceza alabilirdi pr...   \n",
       "2  siyaset    disko lar kaldırılıyor başbakan_yardımcısı ar...   \n",
       "3  siyaset    sarıgül anayasa_mahkemesi ne gidiyor mustafa_...   \n",
       "4  siyaset    erdoğan idamın bir haklılık sebebi var demek ...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  ön seçim vaadi mhp olağan büyük kurultayı nda ...   \n",
       "1  mesut yılmaz yüce divan ceza alabilirdi prof d...   \n",
       "2  disko lar kaldırılıyor başbakan yardımcısı arı...   \n",
       "3  sarıgül anayasa mahkemesi gidiyor mustafa sarı...   \n",
       "4  erdoğan idamın haklılık sebebi demek yeri geld...   \n",
       "\n",
       "                                           text_word  text_word_count  \n",
       "0  [ön, seçim, vaadi, mhp, olağan, büyük, kurulta...              719  \n",
       "1  [mesut, yılmaz, yüce, divan, ceza, alabilirdi,...              379  \n",
       "2  [disko, lar, kaldırılıyor, başbakan, yardımcıs...              575  \n",
       "3  [sarıgül, anayasa, mahkemesi, gidiyor, mustafa...               72  \n",
       "4  [erdoğan, idamın, haklılık, sebebi, demek, yer...              167  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_text = data['cleaned_text'].str\n",
    "data['cleaned_text'] = string_text.translate(str.maketrans('', '', punctuation_set))\n",
    "data['text_word'] = data['cleaned_text'].apply(word_tokenize).to_list()\n",
    "data['text_word_count'] = data['text_word'].apply(len)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-estate",
   "metadata": {},
   "source": [
    "#### LEMMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "understood-classics",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(lemma_words)\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[1;32m---> 15\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_word_stem\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlemmatizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_word_stem_count\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_word_stem\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mlen\u001b[39m)\n\u001b[0;32m     18\u001b[0m data\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\series.py:4357\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4248\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4249\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4252\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4253\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FrameOrSeriesUnion:\n\u001b[0;32m   4254\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4255\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4256\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4355\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4356\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\apply.py:1043\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1040\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m-> 1043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\apply.py:1099\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1093\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   1094\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1095\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[0;32m   1096\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[0;32m   1098\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[1;32m-> 1099\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1100\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1102\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1106\u001b[0m     \u001b[38;5;66;03m# GH 25959 use pd.array instead of tolist\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m     \u001b[38;5;66;03m# so extension arrays can be used\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(pd_array(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\_libs\\lib.pyx:2859\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36mlemmatizer\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      8\u001b[0m lemma_words \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_word\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m---> 10\u001b[0m     lemma_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[43mmorphology\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyzeAndDisambiguate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mbestAnalysis()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mgetLemmas()[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     11\u001b[0m     lemma_words\u001b[38;5;241m.\u001b[39mappend(lemma_word)\n\u001b[0;32m     12\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(lemma_words)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ZEMBEREK_PATH = 'D:\\zemberek-full_old.jar'\n",
    "if jpype.isJVMStarted() is False:\n",
    "        startJVM(getDefaultJVMPath(), '-ea', '-Djava.class.path=%s' % (ZEMBEREK_PATH))\n",
    "TurkishMorphology = JClass('zemberek.morphology.TurkishMorphology')\n",
    "morphology = TurkishMorphology.createWithDefaults()\n",
    "\n",
    "def lemmatizer(text):\n",
    "    lemma_words = []\n",
    "    for text in data['text_word']:\n",
    "        lemma_word = str(morphology.analyzeAndDisambiguate(str(text)).bestAnalysis()[0].getLemmas()[0])\n",
    "        lemma_words.append(lemma_word)\n",
    "    text = ' '.join(lemma_words)\n",
    "    return text\n",
    "\n",
    "data['text_word_stem'] = data['text'].apply(lemmatizer)\n",
    "data['text_word_stem_count'] = data['text_word_stem'].apply(len)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "diverse-planner",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'text_word_stem'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\indexes\\base.py:3361\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3360\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3362\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\_libs\\index.pyx:76\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\_libs\\index.pyx:108\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text_word_stem'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m## maybe can be written in that form but needs to append list first\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# data['text_word_stem'] = [\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#                             morphology.analyzeAndDisambiguate(str(word)).bestAnalysis()[0].getLemmas()[0] \u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#                             for word in data['text_word']\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#                          ]\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# data['text_word_stem_count'] = data['text_word_stem'].apply(len)\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext_word_stem\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\frame.py:3455\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3455\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3457\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\indexes\\base.py:3363\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3361\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3362\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3363\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_scalar(key) \u001b[38;5;129;01mand\u001b[39;00m isna(key) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhasnans:\n\u001b[0;32m   3366\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text_word_stem'"
     ]
    }
   ],
   "source": [
    "## maybe can be written in that form but needs to append list first\n",
    "# data['text_word_stem'] = [\n",
    "#                             morphology.analyzeAndDisambiguate(str(word)).bestAnalysis()[0].getLemmas()[0] \n",
    "#                             for word in data['text_word']\n",
    "#                          ]\n",
    "# data['text_word_stem_count'] = data['text_word_stem'].apply(len)\n",
    "\n",
    "data['text_word_stem']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
