{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "mathematical-vertex",
   "metadata": {},
   "source": [
    "# TOKENIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-flooring",
   "metadata": {},
   "source": [
    "#### Introduction \n",
    "Tokenization is one of the first step in any NLP pipeline. Tokenization is nothing but splitting the raw text into small chunks of words or sentences, called tokens. If the text is split into words, then its called as 'Word Tokenization' and if it's split into sentences then its called as 'Sentence Tokenization'. Generally 'space' is used to perform the word tokenization and characters like 'periods, exclamation point and newline char are used for Sentence Tokenization. We have to choose the appropriate method as per the task in hand. While performing the tokenization few characters like spaces, punctuations are ignored and will not be the part of final list of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-nicaragua",
   "metadata": {},
   "source": [
    "### Tokenization Techniques "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-corrections",
   "metadata": {},
   "source": [
    "**Tokenization Using Python's Inbuilt Method**\n",
    "* Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "liable-stranger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'given', 'text', 'data.', 'We', 'can', 'choose', 'any', 'method', 'based', 'on', 'langauge,', 'library', 'and', 'purpose', 'of', 'modeling.']\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling.\"\"\"\n",
    "tokens = text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-winning",
   "metadata": {},
   "source": [
    "* Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "developmental-onion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Characters like periods, exclamation point and newline char are used to separate the sentences',\n",
       " 'But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method.\"\"\"\n",
    "text.split(\". \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-squad",
   "metadata": {},
   "source": [
    "**Tokenization Using Regular Expressions(RegEx)**\n",
    "* Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "excessive-approach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'given', 'text', 'data', 'We', 'can', 'choose', 'any', 'method', 'based', 'on', 'langauge', 'library', 'and', 'purpose', 'of', 'modeling']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"\"\"There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling.\"\"\"\n",
    "tokens = re.findall(\"[\\w]+\", text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-consideration",
   "metadata": {},
   "source": [
    "* Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "balanced-explanation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Characters like periods, exclamation point and newline char are used to separate the sentences',\n",
       " 'But one drawback with split() method, that we can only use one separator at a time',\n",
       " 'So sentence tonenization wont be foolproof with split() method.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method.\"\"\"\n",
    "tokens_sent = re.compile('[.!?] ').split(text) # Using compile method to combine RegEx patterns\n",
    "tokens_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-freight",
   "metadata": {},
   "source": [
    "**Tokenizaton Using NLTK**\n",
    "* Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "activated-fellowship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'given', 'text', 'data', '.', 'We', 'can', 'choose', 'any', 'method', 'based', 'on', 'langauge', ',', 'library', 'and', 'purpose', 'of', 'modeling', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"\"\"There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling.\"\"\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-scratch",
   "metadata": {},
   "source": [
    "* Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "coated-delaware",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Characters like periods, exclamation point and newline char are used to separate the sentences.',\n",
       " 'But one drawback with split() method, that we can only use one separator at a time!',\n",
       " 'So sentence tonenization wont be foolproof with split() method.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"\"\"Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method.\"\"\"\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-civilization",
   "metadata": {},
   "source": [
    "**Tokenization Using spaCy**\n",
    "* Word Tokenizaton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "structured-calculation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'given', 'text', 'data', '.', 'We', 'can', 'choose', 'any', 'method', 'based', 'on', 'langauge', ',', 'library', 'and', 'purpose', 'of', 'modeling', '.']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "# Load English tokenizer. \n",
    "# nlp object will be used to create 'doc' object which uses preprecoessing pipeline's components such as tagger, parser, NER and word vectors\n",
    "nlp = English()\n",
    "\n",
    "text = \"\"\"There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling.\"\"\"\n",
    "\n",
    "# Now we will process above text using 'nlp' object. Which is use to create documents with linguistic annotations and various nlp properties\n",
    "my_doc = nlp(text)\n",
    "\n",
    "# Above step has already tokenized our text but its in doc format, so lets write fo loop to create list of it\n",
    "token_list = []\n",
    "for token in my_doc:\n",
    "    token_list.append(token.text)\n",
    "\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-latino",
   "metadata": {},
   "source": [
    "* Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "stretch-adams",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Characters like periods, exclamation point and newline char are used to separate the sentences.', 'But one drawback with split() method, that we can only use one separator at a time!', 'So sentence tonenization wont be foolproof with split() method.']\n"
     ]
    }
   ],
   "source": [
    "nlp = English()\n",
    "\n",
    "# Add component to the pipeline\n",
    "nlp.add_pipe('sentencizer')\n",
    "\n",
    "text = \"\"\"Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method.\"\"\"\n",
    "\n",
    "# nlp object is used to create documents with linguistic annotations\n",
    "doc = nlp(text)\n",
    "\n",
    "# Create list of sentence tokens\n",
    "\n",
    "sentence_list =[]\n",
    "for sentence in doc.sents:\n",
    "    sentence_list.append(sentence.text)\n",
    "print(sentence_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-semester",
   "metadata": {},
   "source": [
    "**Tokenization using Keras**\n",
    "* Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "tight-danish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['there', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'given', 'text', 'data', 'we', 'can', 'choose', 'any', 'method', 'based', 'on', 'langauge', 'library', 'and', 'purpose', 'of', 'modeling']\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "text = \"\"\"There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling.\"\"\"\n",
    "\n",
    "tokens = text_to_word_sequence(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-anaheim",
   "metadata": {},
   "source": [
    "* Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "sharp-virginia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['characters like periods, exclamation point and newline char are used to separate the sentences',\n",
       " ' but one drawback with split() method, that we can only use one separator at a time',\n",
       " ' so sentence tonenization wont be foolproof with split() method']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "text = \"\"\"Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method.\"\"\"\n",
    "\n",
    "text_to_word_sequence(text, split= \".\", filters=\"!.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-dominant",
   "metadata": {},
   "source": [
    "**Tokenization using Gensim**\n",
    "* Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "global-bargain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There', 'are', 'multiple', 'ways', 'we', 'can', 'perform', 'tokenization', 'on', 'given', 'text', 'data', 'We', 'can', 'choose', 'any', 'method', 'based', 'on', 'langauge', 'library', 'and', 'purpose', 'of', 'modeling']\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import tokenize\n",
    "\n",
    "text = \"\"\"There are multiple ways we can perform tokenization on given text data. We can choose any method based on langauge, library and purpose of modeling.\"\"\"\n",
    "\n",
    "tokens = list(tokenize(text))\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-latino",
   "metadata": {},
   "source": [
    "* Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eastern-switzerland",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim.summarization'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-fdd9f26a11ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummarization\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextcleaner\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msplit_sentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\"Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplit_sentences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gensim.summarization'"
     ]
    }
   ],
   "source": [
    "from gensim.summarization.textcleaner import split_sentences\n",
    "\n",
    "text = \"\"\"Characters like periods, exclamation point and newline char are used to separate the sentences. But one drawback with split() method, that we can only use one separator at a time! So sentence tonenization wont be foolproof with split() method.\"\"\"\n",
    "\n",
    "list(split_sentences(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "athletic-lender",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: gensim\n",
      "Version: 4.1.2\n",
      "Summary: Python framework for fast Vector Space Modelling\n",
      "Home-page: http://radimrehurek.com/gensim\n",
      "Author: Radim Rehurek\n",
      "Author-email: me@radimrehurek.com\n",
      "License: LGPL-2.1-only\n",
      "Location: c:\\users\\burak\\appdata\\roaming\\python\\python39\\site-packages\n",
      "Requires: Cython, numpy, scipy, smart-open\n",
      "Required-by: scattertext\n"
     ]
    }
   ],
   "source": [
    "!pip show gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-amendment",
   "metadata": {},
   "source": [
    "Gensim.summarization removed after 3.8.3 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
